{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitenvvenv7533db0592054ba080cdf2f4c2630703",
   "display_name": "Python 3.8.1 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse files and convert to JSON format accepted by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(u):\n",
    "\n",
    "    buffer = \"\"\n",
    "    docids = []\n",
    "    doctexts = []\n",
    "    \n",
    "    try:\n",
    "        for filename in os.listdir(u):\n",
    "            with open(os.path.join(path, filename)) as f:\n",
    "                buffer = \"\"\n",
    "                for line in f:\n",
    "                    if \"</DOC>\" in line:\n",
    "                        docid = re.findall(r\"<DOCNO> (.*?) </DOCNO>\", buffer)\n",
    "                        contents = re.findall(r\"<TEXT> (.*?) </TEXT>\", buffer, re.VERBOSE|re.MULTILINE|re.DOTALL)\n",
    "                        contents[0].replace('<TEXT>',' ')\n",
    "                        contents[0].replace('</TEXT>',' ')\n",
    "                        docids.append(docid)\n",
    "                        doctexts.append(contents)\n",
    "                        buffer = \"\"\n",
    "                    line = line.replace('\\n',' ')\n",
    "                    buffer = buffer + line\n",
    "\n",
    "       \n",
    "    except Exception as ex:\n",
    "        print('Exception while parsing')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        # return json.dumps(rec)\n",
    "        return json.dumps([{'_id': docid, 'text': contents} for docid, contents in zip(docids, doctexts)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " path = \"E:\\\\CS 6200\\\\AP_DATA\\\\ap89_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Exception while parsing\nlist index out of range\n"
    }
   ],
   "source": [
    "json_data = parse(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "84678\n"
    }
   ],
   "source": [
    "item_dict = json.loads(json_data)\n",
    "print(len(item_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if _es.ping():\n",
    "        print('Yay Connected')\n",
    "    else:\n",
    "        print('Awww it could not connect!')\n",
    "    return _es\n",
    "if __name__ == '__main__':\n",
    "  logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Yay Connected\n"
    }
   ],
   "source": [
    "es = connect_elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es_object, index_name='assignment1'):\n",
    "    created = False\n",
    "    # index settings\n",
    "    settings = {\n",
    "       \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                },\n",
    "                \"my_stemmer\": {\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"name\": \"english\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped_and_stemmed\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"my_stemmer\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped_and_stemmed\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if not es_object.indices.exists(index_name):\n",
    "            # Ignore 400 means to ignore \"Index Already Exist\" error.\n",
    "            es_object.indices.create(index=index_name, ignore=400, body=settings)\n",
    "            print('Created Index')\n",
    "        created = True\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Created Index\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "create_index(es,'assignment1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in item_dict:\n",
    "    if (len(item['_id'])==1):\n",
    "        item['_id']=item['_id'][0]\n",
    "    if (len(item['text'])==1):\n",
    "        item['text']=item['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertElasticsearch(esobj, bulkItems, doc_type=\"_doc\"):\n",
    "        docs = []\n",
    "        for item in bulkItems:\n",
    "            rec = {'text': item[\"text\"]}\n",
    "            doc = {\n",
    "                '_id': item[\"_id\"],\n",
    "                '_index': 'assignment1',\n",
    "                '_type': doc_type,\n",
    "                '_source': json.dumps(rec)\n",
    "            }\n",
    "            docs.append(doc)\n",
    "\n",
    "        res = helpers.bulk(esobj, actions=docs)\n",
    "        return res \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(84678, [])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "insertElasticsearch(es, item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitenvvenv7533db0592054ba080cdf2f4c2630703",
   "display_name": "Python 3.8.1 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import operator\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if _es.ping():\n",
    "        print('Yay Connected')\n",
    "    else:\n",
    "        print('Awww it could not connect!')\n",
    "    return _es\n",
    "if __name__ == '__main__':\n",
    "  logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = connect_elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Query File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_path = \"D:\\\\CS 6200\\\\AP_DATA\\\\query_desc.51-100.short.txt\"\n",
    "queries = dict()\n",
    "with open(query_file_path) as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "            query_no = re.search(r'\\d+', line).group()\n",
    "            line = line.lstrip('0123456789.- ')\n",
    "            queries[query_no] = line.rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Models"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = es.termvectors(index= \"assignment1\", id = \"AP890110-0294\",term_statistics = True, fields = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 192963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file( file, query_id, doclist, dict):\n",
    "    outF = open(file, \"a\")\n",
    "    string = \"\"\n",
    "    for i in range(0, len(doclist)):\n",
    "        string = str(query_id) + \" Q0 \" + doclist[i] + \"  \" + str(i+1) + \"  \"+str(dict[doclist[i]]) + \" Exp\\n\" \n",
    "        outF.write(string)\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDocList(query):\n",
    "    doclist = []\n",
    "    res = es.search(index=\"assignment1\", body={\"query\": {\"match\": {\"text\": query}}, \"size\":10000})\n",
    "    len_hits = res[\"hits\"][\"total\"][\"value\"]\n",
    "    for i in range(0, len_hits):\n",
    "        doclist.append(res[\"hits\"][\"hits\"][i][\"_id\"])\n",
    "    return doclist"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre compute Corpus Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_dict = dict()\n",
    "for query in queries.values():\n",
    "    analyzerTokens = es.indices.analyze(index = 'assignment1', body = {\"field\": \"text\", \"text\" : query})\n",
    "    query_words = []\n",
    "    unstemmed_words = query.split()\n",
    "\n",
    "    for token in analyzerTokens[\"tokens\"]:\n",
    "        query_words.append(token[\"token\"])\n",
    "\n",
    "    for i in range(0, len(query_words)):\n",
    "        if query_words[i] not in cf_dict:\n",
    "            res = es.search(index=\"assignment1\", body={\"query\": {\"match\": {\"text\": unstemmed_words[i]}}, \"size\":1})\n",
    "            if res[\"hits\"][\"total\"][\"value\"]>0:\n",
    "                document_id = res[\"hits\"][\"hits\"][0][\"_id\"]\n",
    "                print(document_id+\" \"+query_words[i]+\" \\n\")\n",
    "                statistics = es.termvectors(index = \"assignment1\", id = document_id, term_statistics = True, fields=\"text\")\n",
    "                cf_dict[query_words[i]] = statistics['term_vectors']['text']['terms'][query_words[i]]['ttf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unigram LM with Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_jm(query_words, document_list, es_object, index, lam):\n",
    "    lmjm_scores = dict()\n",
    "    statistics = es.mtermvectors(index= index, ids = document_list,term_statistics = False, fields = \"text\")\n",
    "    for doc in statistics['docs']:\n",
    "        lmjm_score = 0\n",
    "        doc_length = 0\n",
    "        tf = 0\n",
    "        if doc[\"term_vectors\"]:\n",
    "            words = doc[\"term_vectors\"][\"text\"][\"terms\"]\n",
    "            termsList = list(words.keys())\n",
    "            for term in termsList:\n",
    "                doc_length += words[term][\"term_freq\"] \n",
    "            for word in query_words:\n",
    "                if word in termsList:\n",
    "                    score = 0\n",
    "                    tf = words[word][\"term_freq\"]\n",
    "                    p_jm = (lam*(tf/doc_length)) + ((1-lam)*cf_dict[word]/vocabSize)\n",
    "                    lmjm_score += math.log(p_jm)\n",
    "                else:\n",
    "                    tf = 0\n",
    "                    lmjm_score += math.log(((1-lam)*cf_dict[word]/vocabSize))\n",
    "            \n",
    "            lmjm_scores[doc[\"_id\"]] = lmjm_score \n",
    "    return lmjm_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_and_write_results(query, query_id, doclist, es_object, index, chunk_size, filename):\n",
    "    # Tokenize query with built in stemmer\n",
    "    analyzerTokens = es.indices.analyze(index = 'assignment1', body = {\"field\": \"text\", \"text\" : query})\n",
    "\n",
    "    query_words = []\n",
    "\n",
    "    for token in analyzerTokens[\"tokens\"]:\n",
    "        query_words.append(token[\"token\"])\n",
    "\n",
    "    dict_lmjm = dict()\n",
    "    dict_lmjm_new = dict()\n",
    "    for i in range(0, len(doclist), chunk_size):\n",
    "        if dict_lmjm:\n",
    "            dict_lmjm_new = dict_lmjm\n",
    "        chunk = doclist[i:i+chunk_size]\n",
    "        if dict_lmjm_new:\n",
    "            dict_lmjm_new.update(unigram_jm(query_words, chunk, es_object, index, 0.5))\n",
    "        else:\n",
    "            dict_lmjm = unigram_jm(query_words, chunk, es_object, index, 0.5)\n",
    "\n",
    "    #Sort scores in descending order\n",
    "    sorted_d = dict( sorted(dict_lmjm_new.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    sortedKeys = list(sorted_d)\n",
    "\n",
    "    # Write scores to File\n",
    "    write_to_file(filename, query_id, sortedKeys[:1000], sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = list(queries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(query_ids)):\n",
    "    run_query_and_write_results(queries[query_ids[i]], query_ids[i], createDocList(queries[query_ids[i]]), es, \"assignment1\", 250, \"D:\\\\CS 6200\\\\AP_DATA\\\\results_jmlm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
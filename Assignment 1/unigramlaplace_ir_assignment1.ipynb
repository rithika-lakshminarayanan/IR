{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitenvvenv7533db0592054ba080cdf2f4c2630703",
   "display_name": "Python 3.8.1 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import operator\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if _es.ping():\n",
    "        print('Yay Connected')\n",
    "    else:\n",
    "        print('Awww it could not connect!')\n",
    "    return _es\n",
    "if __name__ == '__main__':\n",
    "  logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Yay Connected\n"
    }
   ],
   "source": [
    "es = connect_elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Query File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_path = \"D:\\\\CS 6200\\\\AP_DATA\\\\query_desc.51-100.short.txt\"\n",
    "queries = dict()\n",
    "with open(query_file_path) as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "            query_no = re.search(r'\\d+', line).group()\n",
    "            line = line.lstrip('0123456789.- ')\n",
    "            queries[query_no] = line.rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Models"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = es.termvectors(index= \"assignment1\", id = \"AP890110-0294\",term_statistics = True, fields = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_length = statistics[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"]/statistics[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = 192963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_documents = statistics[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file( file, query_id, doclist, dict):\n",
    "    outF = open(file, \"a\")\n",
    "    string = \"\"\n",
    "    for i in range(0, len(doclist)):\n",
    "        string = str(query_id) + \" Q0 \" + doclist[i] + \"  \" + str(i+1) + \"  \"+str(dict[doclist[i]]) + \" Exp\\n\" \n",
    "        outF.write(string)\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDocList(query):\n",
    "    doclist = []\n",
    "    res = es.search(index=\"assignment1\", body={\"query\": {\"match\": {\"text\": query}}, \"size\":10000})\n",
    "    len_hits = res[\"hits\"][\"total\"][\"value\"]\n",
    "    for i in range(0, len_hits):\n",
    "        doclist.append(res[\"hits\"][\"hits\"][i][\"_id\"])\n",
    "    return doclist"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unigram LM with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_laplace(query_words, document_list, es_object, index):\n",
    "    lmlaplace_scores = dict()\n",
    "    statistics = es.mtermvectors(index= index, ids = document_list,term_statistics = False, fields = \"text\")\n",
    "    for doc in statistics['docs']:\n",
    "        lmlaplace_score = 0\n",
    "        prior = -9999\n",
    "        doc_length = 0\n",
    "        tf = 0\n",
    "        if doc[\"term_vectors\"]:\n",
    "            words = doc[\"term_vectors\"][\"text\"][\"terms\"]\n",
    "            termsList = list(words.keys())\n",
    "            for term in termsList:\n",
    "                doc_length += words[term][\"term_freq\"] \n",
    "            for word in query_words:\n",
    "                if word in termsList:\n",
    "                    score = 0\n",
    "                    tf = words[word][\"term_freq\"]\n",
    "                    p_laplace = (tf + 1)/(doc_length + vocabSize)\n",
    "                    lmlaplace_score += math.log(p_laplace)\n",
    "                else:\n",
    "                    tf = 0\n",
    "                    lmlaplace_score += prior\n",
    "            \n",
    "            lmlaplace_scores[doc[\"_id\"]] = lmlaplace_score\n",
    "    return lmlaplace_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_and_write_results(query, query_id, doclist, es_object, index, chunk_size, filename):\n",
    "    # Tokenize query with built in stemmer\n",
    "    analyzerTokens = es.indices.analyze(body = {\"tokenizer\" : \"standard\", \"filter\" : [\"lowercase\", \"stemmer\"], \"text\" : query})\n",
    "\n",
    "    query_words = []\n",
    "\n",
    "    for token in analyzerTokens[\"tokens\"]:\n",
    "        query_words.append(token[\"token\"])\n",
    "\n",
    "    dict_lmlaplace = dict()\n",
    "    dict_lmlaplace_new = dict()\n",
    "    for i in range(0, len(doclist), chunk_size):\n",
    "        if dict_lmlaplace:\n",
    "            dict_lmlaplace_new = dict_lmlaplace\n",
    "        chunk = doclist[i:i+chunk_size]\n",
    "        if dict_lmlaplace_new:\n",
    "            dict_lmlaplace_new.update(unigram_laplace(query_words, chunk, es_object, index))\n",
    "        else:\n",
    "            dict_lmlaplace = unigram_laplace(query_words, chunk, es_object, index)\n",
    "\n",
    "    #Sort scores in descending order\n",
    "    sorted_d = dict( sorted(dict_lmlaplace_new.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    sortedKeys = list(sorted_d)\n",
    "\n",
    "    # Write scores to File\n",
    "    write_to_file(filename, query_id, sortedKeys[:1000], sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = list(queries.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(query_ids)):\n",
    "    run_query_and_write_results(queries[query_ids[i]], query_ids[i], createDocList(queries[query_ids[i]]), es, \"assignment1\", 250, \"D:\\\\CS 6200\\\\AP_DATA\\\\results_laplacelm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
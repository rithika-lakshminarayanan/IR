{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitenvvenv15e99bfcd1ba46ce8cf4ba06f7193b01",
   "display_name": "Python 3.8.1 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "query_file_path = \"E:\\\\CS 6200\\\\AP_DATA\\\\query_desc.51-100.short.txt\"\n",
    "queries = dict()\n",
    "with open(query_file_path) as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "            query_no = re.search(r'\\d+', line).group()\n",
    "            line = line.lstrip('0123456789.- ')\n",
    "            if query_no not in queries:\n",
    "                queries[query_no] = line.rstrip().replace('-', ' ').replace('.', '').replace('(', '').replace(')', '').replace('\"', '').replace(',', '')\n",
    "            # print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_file(file_name):\n",
    "    path = 'E:\\\\CS 6200\\\\Assignment 2\\\\stemmed'\n",
    "    with open(os.path.join(path, file_name), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = unpickle_file('l85_catalog.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = unpickle_file('doc_lengths.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_doc_length(document_lengths):\n",
    "    sum = 0\n",
    "    num_of_docs = len(list(document_lengths.keys()))\n",
    "    for doc in doc_lengths:\n",
    "        sum += document_lengths[doc]\n",
    "    return (sum/num_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length = avg_doc_length(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_to_num = dict()\n",
    "num_to_docid = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docid_mapping(id_mapping_file):\n",
    "    path = 'E:\\\\CS 6200\\\\AP_DATA'\n",
    "    with open(os.path.join(path, id_mapping_file), 'r') as f:\n",
    "        for line in f:\n",
    "            doc_mapping = line.split()\n",
    "            docid_to_num[doc_mapping[0]] = doc_mapping[1]\n",
    "            num_to_docid[doc_mapping[1]] = doc_mapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_docid_mapping('docmapping.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_length(docnum, doc_lengths):\n",
    "    return doc_lengths[docnum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = len(list(doc_lengths.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "84678\n"
    }
   ],
   "source": [
    "print(total_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:\\\\CS 6200\\\\AP_DATA\\\\stoplist.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        stopWords.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttf(word):\n",
    "    inv_file = 'E:\\\\CS 6200\\\\Assignment 2\\\\stemmed\\\\l85_merged_index.txt'\n",
    "\n",
    "    with open(inv_file, 'r') as f:\n",
    "        f.seek(catalog[word][0])\n",
    "        line = f.read(catalog[word][1]) \n",
    "        line = re.sub('^.*:', '', line)\n",
    "        line = line.replace(';','')\n",
    "        numbers_list = line.split(',')\n",
    "        docid = -1\n",
    "        cursor = 0\n",
    "        ttf = 0\n",
    "        doc_freq = 0\n",
    "        positions = []\n",
    "        docids = []\n",
    "        tf = -1\n",
    "        for i in range(0, len(numbers_list)):\n",
    "            if docid == -1:\n",
    "                # print('Found a docid')\n",
    "                docid = numbers_list[i]\n",
    "            elif docid != -1 and tf == -1:\n",
    "                # print('Found the TF')\n",
    "                tf = numbers_list[i]\n",
    "            elif docid != -1 and tf != -1:\n",
    "                # print('Found the positions')\n",
    "                positions.append(numbers_list[i])\n",
    "                cursor += 1\n",
    "                if int(cursor) == int(tf):\n",
    "                    cursor = 0\n",
    "                    ttf += int(tf)\n",
    "                    docid = -1\n",
    "                    tf = -1\n",
    "                    positions = []\n",
    "\n",
    "    # print(ttf)\n",
    "\n",
    "    return ttf\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5907\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5907"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "get_ttf('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_statistics(query, inverted_file_name):\n",
    "    statistics_dict = dict()\n",
    "    statistics_dict['docs'] = dict()\n",
    "    path = 'E:\\\\CS 6200\\\\Assignment 2\\\\stemmed'\n",
    "    \n",
    "    query_words = []\n",
    "    query = query.lower()\n",
    "    tokenized_words = word_tokenize(query)\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopWords:\n",
    "            query_words.append(ps.stem(word))\n",
    "    with open(os.path.join(path, inverted_file_name), 'r') as f:\n",
    "        for word in query_words:\n",
    "            f.seek(catalog[word][0])\n",
    "            # Returns line in format 'word:docid,len,positions,docid,len,positions...'\n",
    "            line = f.read(catalog[word][1]) \n",
    "            # print(line)\n",
    "            line = re.sub('^.*:', '', line)\n",
    "            line = line.replace(';','')\n",
    "            numbers_list = line.split(',')\n",
    "            # print(len(numbers_list))\n",
    "            docid = -1\n",
    "            cursor = 0\n",
    "            ttf = 0\n",
    "            doc_freq = 0\n",
    "            positions = []\n",
    "            docids = []\n",
    "            tf = -1\n",
    "            for i in range(0, len(numbers_list)):\n",
    "                if docid == -1:\n",
    "                    # print('Found a docid')\n",
    "                    docid = numbers_list[i]\n",
    "                elif docid != -1 and tf == -1:\n",
    "                    # print('Found the TF')\n",
    "                    tf = numbers_list[i]\n",
    "                elif docid != -1 and tf != -1:\n",
    "                    # print('Found the positions')\n",
    "                    positions.append(numbers_list[i])\n",
    "                    cursor += 1\n",
    "                    if int(cursor) == int(tf):\n",
    "                        cursor = 0\n",
    "                        ttf += int(tf)\n",
    "                        doc_freq += 1\n",
    "                        # print('Adding to dict...')\n",
    "                        # print(docid, tf, positions)\n",
    "                        doc_original_id = num_to_docid[docid]\n",
    "                        if doc_original_id not in statistics_dict['docs']:\n",
    "                            statistics_dict['docs'][doc_original_id] = dict()\n",
    "                        if 'term_statistics' not in statistics_dict['docs'][doc_original_id]:\n",
    "                            statistics_dict['docs'][doc_original_id]['term_statistics'] = dict()\n",
    "                        if word not in statistics_dict['docs'][doc_original_id]['term_statistics']:\n",
    "                            statistics_dict['docs'][doc_original_id]['term_statistics'][word] = dict()\n",
    "                        statistics_dict['docs'][doc_original_id]['term_statistics'][word]['term_freq'] = tf\n",
    "                        statistics_dict['docs'][doc_original_id]['term_statistics'][word]['positions'] = positions\n",
    "                        statistics_dict['docs'][doc_original_id]['doc_length'] = get_doc_length(int(docid), doc_lengths)\n",
    "                        docids.append(doc_original_id)\n",
    "                        docid = -1\n",
    "                        tf = -1\n",
    "                        positions = []\n",
    "                        # print(statistics_dict)\n",
    "            for doc_original_id in docids:\n",
    "                # print(doc_original_id)\n",
    "                statistics_dict['docs'][doc_original_id]['term_statistics'][word]['ttf'] = ttf\n",
    "                statistics_dict['docs'][doc_original_id]['term_statistics'][word]['doc_freq'] = doc_freq\n",
    "            ttf = 0\n",
    "            doc_freq = 0\n",
    "    return statistics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query):\n",
    "    tfidf_scores = dict()\n",
    "\n",
    "    query_words = []\n",
    "    query = query.lower()\n",
    "    tokenized_words = word_tokenize(query)\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopWords:\n",
    "            query_words.append(ps.stem(word))\n",
    "\n",
    "    statistics = fetch_statistics(query, 'l85_merged_index.txt')\n",
    "\n",
    "    for doc in statistics['docs']:\n",
    "        doc_frequency = 1\n",
    "        doc_length = 0\n",
    "        okapiScore = 0\n",
    "        tfidf_score = 0\n",
    "        tf = 0\n",
    "        if statistics['docs'][doc]['term_statistics']:\n",
    "            words = statistics['docs'][doc]['term_statistics']\n",
    "            termsList = list(words.keys())\n",
    "            doc_length = int(statistics['docs'][doc]['doc_length']) \n",
    "            for word in query_words:\n",
    "                if word in termsList:\n",
    "                    tf = int(words[word][\"term_freq\"])\n",
    "                    doc_frequency = int(words[word][\"doc_freq\"])\n",
    "                else:\n",
    "                    tf = 0\n",
    "                    doc_frequency = 1\n",
    "                okapiScore = tf/(tf + 0.5 + ((1.5*doc_length)/avg_length))\n",
    "                # idf = math.log((total_docs/doc_frequency))\n",
    "                tfidf_score += okapiScore \n",
    "            tfidf_scores[doc] = tfidf_score \n",
    "    return tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_bm25(query, b, k1):\n",
    "    bm25_scores = dict()\n",
    "\n",
    "    query_words = []\n",
    "    query = query.lower()\n",
    "    tokenized_words = word_tokenize(query)\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopWords:\n",
    "            query_words.append(ps.stem(word))\n",
    "\n",
    "    statistics = fetch_statistics(query, 'l85_merged_index.txt')\n",
    "    \n",
    "    for doc in statistics['docs']:\n",
    "        doc_frequency = 0\n",
    "        okapibm25_score = 0\n",
    "        tf = 0\n",
    "        if statistics['docs'][doc]['term_statistics']:\n",
    "            words = statistics['docs'][doc]['term_statistics']\n",
    "            termsList = list(words.keys())\n",
    "            doc_length = int(statistics['docs'][doc]['doc_length']) \n",
    "            for word in query_words:\n",
    "                if word in termsList:\n",
    "                    tf = int(words[word][\"term_freq\"])\n",
    "                    doc_frequency = int(words[word][\"doc_freq\"])\n",
    "                else:\n",
    "                    tf = 0\n",
    "                    doc_frequency = 1\n",
    "                okapiScore = (tf + (k1*tf))/(tf + (k1*((1-b) + (b*(doc_length/avg_length)))))\n",
    "                idf = math.log((total_docs + 0.5/doc_frequency + 0.5))\n",
    "                okapibm25_score += okapiScore * idf\n",
    "            bm25_scores[doc] = okapibm25_score\n",
    "    return bm25_scores\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(list(catalog.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_laplace(query, vocab_size):\n",
    "    lmlaplace_scores = dict()\n",
    "    query_words = []\n",
    "    query = query.lower()\n",
    "    tokenized_words = word_tokenize(query)\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopWords:\n",
    "            query_words.append(ps.stem(word))\n",
    "\n",
    "\n",
    "    statistics = fetch_statistics(query, 'l85_merged_index.txt')\n",
    "    for doc in statistics['docs']:\n",
    "        tf = 0\n",
    "        laplace_score = 0\n",
    "        doc_length = 0\n",
    "        if statistics['docs'][doc]['term_statistics']:\n",
    "            words = statistics['docs'][doc]['term_statistics']\n",
    "            termsList = list(words.keys())\n",
    "            doc_length = int(statistics['docs'][doc]['doc_length']) \n",
    "            for word in query_words:\n",
    "                if word in termsList:\n",
    "                    tf = int(words[word][\"term_freq\"])\n",
    "                else:\n",
    "                    tf = 0\n",
    "                p_laplace = (tf + 1)/(doc_length + vocab_size)\n",
    "                laplace_score += math.log(p_laplace)\n",
    "\n",
    "            lmlaplace_scores[doc] = laplace_score\n",
    "    return lmlaplace_scores\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "document\ndiscuss\nalleg\nmeasur\ntaken\ncorrupt\npublic\noffici\ngovernment\njurisdict\nworldwid\ndocument\nreport\ntype\nweather\nevent\ndirectli\ncaus\nleast\nfatal\nlocat\ndocument\npredict\nprime\nlend\nrate\nreport\nactual\nprime\nrate\nmove\ndocument\nreport\nincurs\nland\nair\nwater\nborder\narea\ncountri\nmilitari\nforc\nsecond\ncountri\nguerrilla\ngroup\nbase\nsecond\ncountri\ndocument\nreport\nevent\nresult\npolit\nmotiv\nhostag\ntake\ndocument\nreport\nmilitari\ncoup\nd'etat\nattempt\nsuccess\ncountri\ndocument\ndescrib\nidentifi\nsupport\nnation\nrifl\nassoci\nnra\nasset\ndocument\nidentifi\ndevelop\niran\ncontra\naffair\ndocument\npredict\nanticip\nrail\nstrike\nreport\nongo\nrail\nstrike\ndocument\nreport\npoach\nmethod\ntype\nwildlif\ndocument\ncite\nsign\ncontract\npreliminari\nagreement\nmake\ntent\nreserv\nlaunch\ncommerci\nsatellit\ndocument\nreport\ncurrent\ncrimin\naction\noffic\nfail\nfinanci\ninstitut\ndocument\nidentifi\ncrime\nperpetr\naid\ncomput\ndocument\nidentifi\neffort\nnon\ncommunist\nindustri\nstate\nregul\ntransfer\nhigh\ntech\ngood\ntechnolog\nundesir\nnation\ndocument\nidentifi\nexist\npend\ninvest\nopec\nmember\nstate\ndownstream\noper\ndocument\ndiscuss\nrole\nisrael\niran\ncontra\naffair\ndocument\ndescrib\ncomput\napplic\ncrime\nsolv\ndocument\nreport\nactual\nstudi\nunsubstanti\nconcern\nsafeti\nmanufactur\nemploye\ninstal\nworker\nfine\ndiamet\nfiber\ninsul\nproduct\ndocument\ndiscuss\nmci\nbell\nsystem\nbreakup\ndocument\nidentifi\ninstanc\nfiber\noptic\ntechnolog\nactual\ndocument\nidentifi\nindividu\norgan\nproduc\nfiber\noptic\nequip\ndocument\ndescrib\nside\ncontroversi\nstandard\nperform\ndetermin\nsalari\nlevel\nincent\npay\ncontrast\ndetermin\npay\nsole\nbasi\nsenior\nlongev\njob\ndocument\nidentifi\nplatform\n1988\npresidenti\ncandid\ndocument\nidentifi\nmachin\ntranslat\nsystem\ndocument\nidentifi\nacquisit\narmi\nspecifi\nadvanc\nweapon\nsystem\n"
    }
   ],
   "source": [
    "ttf_dict = {}\n",
    "\n",
    "for query in queries:\n",
    "    tokenized_words = word_tokenize(queries[query].lower())\n",
    "\n",
    "    query_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopWords:\n",
    "            query_words.append(ps.stem(word))\n",
    "\n",
    "    for word in query_words:\n",
    "        print(word)\n",
    "        if word not in ttf_dict:\n",
    "            ttf_dict[word] = get_ttf(word) \n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "267"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "ttf_dict['mci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_jm(query, vocab_size, lam):\n",
    "\n",
    "    jm_scores = dict()\n",
    "    query_words = []\n",
    "    query = query.lower()\n",
    "    tokenized_words = word_tokenize(query)\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopWords:\n",
    "            query_words.append(ps.stem(word))\n",
    "\n",
    "\n",
    "    statistics = fetch_statistics(query, 'l85_merged_index.txt')\n",
    "    for doc in statistics['docs']:\n",
    "        # print('DOC:', doc)\n",
    "        tf = 0\n",
    "        lmjm_score = 0\n",
    "        doc_length = 0\n",
    "        if statistics['docs'][doc]['term_statistics']:\n",
    "            words = statistics['docs'][doc]['term_statistics']\n",
    "            termsList = list(words.keys())\n",
    "            doc_length = int(statistics['docs'][doc]['doc_length']) \n",
    "            for word in query_words:\n",
    "                ttf = ttf_dict[word]\n",
    "                # print('Size, word: ',len(query_words), word)\n",
    "                if word in termsList:\n",
    "                    tf = int(words[word][\"term_freq\"])\n",
    "                    p_jm = (lam*(tf/doc_length)) + ((1-lam)* ttf/vocab_size)\n",
    "                    # print('PJM: ', p_jm)\n",
    "                    lmjm_score += math.log(p_jm)\n",
    "                else:\n",
    "                    # print(word)\n",
    "                    lmjm_score += math.log(((1-lam)* ttf/vocab_size))\n",
    "\n",
    "            jm_scores[doc] = lmjm_score\n",
    "    print('Exiting JM function')\n",
    "    return jm_scores\n",
    "    # jm_scores = dict()\n",
    "    # query_words = []\n",
    "    # query = query.lower()\n",
    "    # tokenized_words = word_tokenize(query)\n",
    "\n",
    "    # for word in tokenized_words:\n",
    "    #     if word not in stopWords:\n",
    "    #         query_words.append(ps.stem(word))\n",
    "\n",
    "    # print('Initializing...')\n",
    "    # # print('Query words: ', query_words)\n",
    "\n",
    "\n",
    "    # statistics = fetch_statistics(query, 'l85_merged_index.txt')\n",
    "    # for doc in statistics['docs']:\n",
    "    #     tf = 0\n",
    "    #     lmjm_score = 0\n",
    "    #     doc_length = 0\n",
    "    #     if statistics['docs'][doc]['term_statistics']:\n",
    "    #         words = statistics['docs'][doc]['term_statistics']\n",
    "    #         termsList = list(words.keys())\n",
    "    #         doc_length = int(statistics['docs'][doc]['doc_length'])\n",
    "    #         print('Query: ', query_words)\n",
    "\n",
    "    #         for word in query_words:\n",
    "    #             print('Query word ', word)\n",
    "    #             if word in termsList:\n",
    "    #                 tf = int(words[word][\"term_freq\"])\n",
    "    #                 p_jm = (lam*(tf/doc_length)) + ((1-lam)* tf/vocab_size)\n",
    "    #                 lmjm_score += math.log(p_jm)\n",
    "    #             else:\n",
    "    #                 lmjm_score += math.log(((1-lam)* tf/vocab_size))\n",
    "\n",
    "    #         jm_scores[doc] = lmjm_score\n",
    "    # return jm_scores\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file( file, query_id, doclist, dict):\n",
    "    outF = open(file, \"a\")\n",
    "    string = \"\"\n",
    "    for i in range(0, len(doclist)):\n",
    "        string = str(query_id) + \" Q0 \" + doclist[i] + \"  \" + str(i+1) + \"  \"+str(dict[doclist[i]]) + \" Exp\\n\" \n",
    "        outF.write(string)\n",
    "    outF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_and_write_results(query, query_id, filename, choice):\n",
    "    score_dict = dict()\n",
    "    if choice == 1:\n",
    "        score_dict = tf_idf(query)\n",
    "    elif choice == 2:\n",
    "        score_dict = okapi_bm25(query, 0.75, 1.2)\n",
    "    elif choice == 3:\n",
    "        score_dict = unigram_laplace(query, vocab_size)\n",
    "    elif choice == 4:\n",
    "        score_dict = unigram_jm(query, vocab_size, 0.95)\n",
    "\n",
    "    sorted_d = dict( sorted(score_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    sortedKeys = list(sorted_d)\n",
    "\n",
    "    # Write scores to File\n",
    "    write_to_file(filename, query_id, sortedKeys, sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = list(queries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(query_ids)):\n",
    "#     run_query_and_write_results(queries[query_ids[i]], query_ids[i], 'E:\\\\CS 6200\\\\Assignment 2\\\\results\\\\a6_tfidf_stemmed.txt', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(query_ids)):\n",
    "#     run_query_and_write_results(queries[query_ids[i]], query_ids[i], 'E:\\\\CS 6200\\\\Assignment 2\\\\results\\\\a6_bm25_stemmed.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, len(query_ids)):\n",
    "#     run_query_and_write_results(queries[query_ids[i]], query_ids[i], 'E:\\\\CS 6200\\\\Assignment 2\\\\results\\\\a6_laplace_stemmed.txt', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram JM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Query  0\nExiting JM function\nQuery  1\nExiting JM function\nQuery  2\nExiting JM function\nQuery  3\nExiting JM function\nQuery  4\nExiting JM function\nQuery  5\nExiting JM function\nQuery  6\nExiting JM function\nQuery  7\nExiting JM function\nQuery  8\nExiting JM function\nQuery  9\nExiting JM function\nQuery  10\nExiting JM function\nQuery  11\nExiting JM function\nQuery  12\nExiting JM function\nQuery  13\nExiting JM function\nQuery  14\nExiting JM function\nQuery  15\nExiting JM function\nQuery  16\nExiting JM function\nQuery  17\nExiting JM function\nQuery  18\nExiting JM function\nQuery  19\nExiting JM function\nQuery  20\nExiting JM function\nQuery  21\nExiting JM function\nQuery  22\nExiting JM function\nQuery  23\nExiting JM function\nQuery  24\nExiting JM function\n"
    }
   ],
   "source": [
    "for i in range(0, len(query_ids)):\n",
    "    print('Query ', i)\n",
    "    run_query_and_write_results(queries[query_ids[i]], query_ids[i], 'E:\\\\CS 6200\\\\Assignment 2\\\\results\\\\a6_jm5_stemmed.txt', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}